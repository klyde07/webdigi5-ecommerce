name: Automated Courir Scraping

on:
  schedule:
    - cron: '*/30 * * * *'  # Toutes les 30 minutes

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE: ${{ secrets.SUPABASE_SERVICE_ROLE }}
      PYTHONPATH: /usr/bin/python3

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install -r scripts/requirements.txt

    - name: Run scraping
      run: |
        cd scripts
        python scraper.py

    - name: Upload logs
      uses: actions/upload-artifact@v3
      with:
        name: scraping-logs
        path: scripts/scraping.log
        retention-days: 7

    - name: Notify on failure
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: 'ðŸš¨ Scraping Failed - Courir.com',
            body: 'The automated scraping job failed. Please check the logs.',
            labels: ['scraping', 'failure']
          })
