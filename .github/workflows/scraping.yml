name: ğŸš€ Scraping Automatique Courir.com

on:
  schedule:
    - cron: '0 */6 * * *'  # â° Toutes les 6 heures
  workflow_dispatch:         # ğŸ® DÃ©clenchement manuel

jobs:
  scrape:
    runs-on: ubuntu-latest   # ğŸ’» Machine virtuelle Ubuntu
    timeout-minutes: 30      # â±ï¸ AugmentÃ© Ã  30 minutes pour plus de marge

    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE: ${{ secrets.SUPABASE_SERVICE_ROLE }}  # AlignÃ© avec scraper.py

    steps:
    # Ã‰tape 1: ğŸ“¥ TÃ©lÃ©charger le code
    - name: Checkout du code
      uses: actions/checkout@v4

    # Ã‰tape 2: ğŸ Installer Python
    - name: Installation de Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    # Ã‰tape 3: ğŸ“¦ Installer Chrome et Chromedriver (version compatible)
    - name: Installer Chrome et Chromedriver
      run: |
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        CHROMEDRIVER_VERSION=140.0.7339.185
        wget -N https://storage.googleapis.com/chrome-for-testing-public/${CHROMEDRIVER_VERSION}/linux64/chromedriver-linux64.zip -P ~/
        unzip ~/chromedriver-linux64.zip -d ~/
        sudo mv ~/chromedriver-linux64/chromedriver /usr/local/bin/
        sudo chmod +x /usr/local/bin/chromedriver

    # Ã‰tape 4: ğŸ“¦ Mettre en cache les dÃ©pendances Python
    - name: Cache des dÃ©pendances
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('scripts/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    # Ã‰tape 5: ğŸ“¦ Installer les dÃ©pendances
    - name: Installation des dÃ©pendances
      run: pip install -r scripts/requirements.txt

    # Ã‰tape 6: ğŸ•·ï¸ Lancer le scraping
    - name: ExÃ©cution du scraping
      run: |
        cd scripts
        python scraper.py || exit 1  # Exit avec erreur si le script Ã©choue
