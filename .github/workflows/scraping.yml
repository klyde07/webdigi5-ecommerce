name: 🚀 Scraping Automatique Courir.com

on:
  schedule:
    - cron: '0 */6 * * *'  # ⏰ Toutes les 6 heures
  workflow_dispatch:         # 🎮 Déclenchement manuel

jobs:
  scrape:
    runs-on: ubuntu-latest   # 💻 Machine virtuelle Ubuntu
    timeout-minutes: 30      # ⏱️ Augmenté à 30 minutes pour plus de marge

    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE: ${{ secrets.SUPABASE_SERVICE_ROLE }}  # Aligné avec scraper.py

    steps:
    # Étape 1: 📥 Télécharger le code
    - name: Checkout du code
      uses: actions/checkout@v4

    # Étape 2: 🐍 Installer Python
    - name: Installation de Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    # Étape 3: 📦 Installer Chrome et Chromedriver (version compatible)
    - name: Installer Chrome et Chromedriver
      run: |
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        CHROMEDRIVER_VERSION=140.0.7339.185
        wget -N https://storage.googleapis.com/chrome-for-testing-public/${CHROMEDRIVER_VERSION}/linux64/chromedriver-linux64.zip -P ~/
        unzip ~/chromedriver-linux64.zip -d ~/
        sudo mv ~/chromedriver-linux64/chromedriver /usr/local/bin/
        sudo chmod +x /usr/local/bin/chromedriver

    # Étape 4: 📦 Mettre en cache les dépendances Python
    - name: Cache des dépendances
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('scripts/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    # Étape 5: 📦 Installer les dépendances
    - name: Installation des dépendances
      run: pip install -r scripts/requirements.txt

    # Étape 6: 🕷️ Lancer le scraping
    - name: Exécution du scraping
      run: |
        cd scripts
        python scraper.py || exit 1  # Exit avec erreur si le script échoue
